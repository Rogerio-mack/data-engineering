{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdU9aSDILB2CspgGXFTe1y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rogerio-mack/data-engineering/blob/main/Aula_08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/Rogerio-mack/data-engineering/blob/main/mack_logo.png?raw=true\" height=\"70\" align=\"right\"/></a>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ifRIxMqtyJGc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/Rogerio-mack/data-engineering/blob/main/figs/HadoopCover.png?raw=true\" height=\"250\" align=\"left\"/></a>"
      ],
      "metadata": {
        "id": "-q7Ye_WPLled"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sistemas de Armazenamento de Dados\n",
        "\n",
        "## File Storage\n",
        "Armazenamento gerenciado por sistemas de arquivos operacionais, organizando dados em diretórios e arquivos. NTFS (Windows), ext4 (Linux). Escalabilidade limitada e dificuldade em gerenciar grandes volumes de dados. Dados armazenados em HDDs ou SSDs e não são abstraídos.\n",
        "\n",
        "## Block Storage\n",
        "Divide dados em blocos de tamanho fixo, gerenciados de forma independente.\n",
        "Comum em virtualização e bancos de dados que exigem alta performance.\n",
        "Amazon EBS, Google Persistent Disk. Baixa latência para recuperação e atualização dos dados torna esse sistema ideal para ambientes intensivos em atualização. Servidores de e-mail, além de bancos de dados, em geral empregam esta opção. Dados armazenados em HDDs ou SSDs e não são abstraídos.\n",
        "\n",
        "## **Object Storage**\n",
        "Armazena dados como objetos, incluindo metadados e uma chave única para acesso.\n",
        "Dados não estruturados, como imagens, som e vídeo. Amazon S3, Google Cloud Storage. Escalabilidade e facilidade de gerenciamento de grandes volumes de dados, mas com latência maior em comparação com armazenamento em bloco e complexidade na recuperação de dados. O identificador único (em geral 128 bits)desempenha um papel fundamental na localização do objeto.\n",
        "\n",
        "Documentos, fotos, e-mails (histórico), principalmente quando precisam ter sua integridade preservada (não atualização), são casos de uso importantes aqui. O acesso aos dados é feito em geral por APIs. Versionamento, criptografia, mecanismos de segurança, API de acesso internet, replicação e gerenciamento do ciclo de vida dos dados são normalmente associados a essa classe de armazenamento.\n",
        "\n",
        "## Cache and Memory\n",
        "Redis, Memcached, são exemplos. Mas, embora aumente significativamente a velocidade de acesso a dados, apresenta capacidade limitada e volatilidade dos dados armazenados.\n",
        "\n",
        "## **Hadoop Distributed File System (HDFS)**\n",
        "Sistema de arquivos distribuído projetado para armazenar grandes volumes de dados em clusters. Projetado para processamento de big data e análise de grandes conjuntos de dados. Tem alta tolerância a falhas e escalabilidade horizontal, mas é de grande complexidade para configuração e manutenção. Em geral também oferece criptografia, mas a segurança no acesso aos dados é limitada.\n",
        "\n",
        "Casos de uso comuns são o armazenamento de dados e operações ETL, processamento e análise de logs, análise de dados e aprendizado de máquina em larga escala. Em parte, esses casos de uso são viabilizados pela integração com as demais ferramentas do ecossistema Hadoop, como MapReduce, Hive e Pig.\n",
        "\n",
        "## Streaming\n",
        "Processamento contínuo de dados em tempo real para análise em tempo real, como monitoramento de eventos, logs. Apache Kafka, Apache Flink.\n",
        "\n",
        "## Bancos de Dados (*)\n",
        "(já vimos bastante)\n",
        "\n"
      ],
      "metadata": {
        "id": "kZC6LAWFUMTs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "| **Tipo de Armazenamento** | **AWS** | **Google Cloud** | **Azure** |\n",
        "|---------------------------|---------|------------------|-----------|\n",
        "| **Arquivos (File Storage)** | EFS (Elastic File System), FSx | Filestore | Azure Files |\n",
        "| **Blocos (Block Storage)** | EBS (Elastic Block Store) | Persistent Disks | Azure Managed Disks |\n",
        "| **Objetos (Object Storage)** | S3 (Simple Storage Service) | Cloud Storage | Azure Blob Storage |\n",
        "| **HDFS (Hadoop Distributed File System)** | EMR (Elastic MapReduce) com HDFS | Dataproc com HDFS | HDInsight com HDFS |\n",
        "| **Bancos de Dados Relacionais (SQL)** | RDS (Relational Database Service), Aurora | Cloud SQL, AlloyDB | Azure SQL Database |\n",
        "| **Bancos de Dados Não Relacionais (NoSQL)** | DynamoDB | Firestore, Bigtable | Azure Cosmos DB |\n",
        "| **Data Warehouse** | Redshift | BigQuery | Azure Synapse Analytics |\n",
        "| **Armazenamento de Streaming** | Kinesis | Pub/Sub | Azure Event Hubs |\n",
        "| **Armazenamento em Cache** | ElastiCache (Redis/Memcached) | Memorystore (Redis/Memcached) | Azure Cache for Redis |\n",
        "\n"
      ],
      "metadata": {
        "id": "-KvchPsfeLC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eco-Sistema Hadoop"
      ],
      "metadata": {
        "id": "DEhT1S7RfNG8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/Rogerio-mack/data-engineering/blob/main/figs/Hadoop.png?raw=true\" height=\"300\" align=\"left\"/></a>\n",
        "\n"
      ],
      "metadata": {
        "id": "4hOM7YoCu0V3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/Rogerio-mack/data-engineering/blob/main/figs/Hadoopdependencies.png?raw=true\" height=\"500\" align=\"left\"/></a>"
      ],
      "metadata": {
        "id": "3jfXU5-0wo1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Propósito        | Componente   | Descrição                                                                 |\n",
        "|------------------|--------------|---------------------------------------------------------------------------|\n",
        "| **Armazenamento**    | **HDFS**         | Sistema de arquivos distribuído que armazena grandes volumes de dados em clusters. |\n",
        "| Armazenamento    | HBase        | Banco de dados NoSQL, orientado a colunas, construído sobre HDFS, ideal para dados não estruturados. |\n",
        "| **Processamento**    | **MapReduce**    | Framework de processamento distribuído que executa tarefas em paralelo no cluster. |\n",
        "| **Processamento**    | **Apache Spark** | Plataforma de processamento rápido e generalizado, com suporte a batch e streaming, que pode substituir o MapReduce. |\n",
        "| Processamento    | Apache Impala| Motor de consultas SQL em tempo real que permite análise de dados diretamente no HDFS e HBase com baixa latência. |\n",
        "| **Orquestração**     | **YARN**         | Sistema de gerenciamento de recursos e agendamento de tarefas no cluster. |\n",
        "| **Armazenamento**    | **Hive**         | Sistema de data warehouse que oferece consultas SQL-like para dados armazenados no HDFS. |\n",
        "| Streaming        | Apache Flume | Serviço para coleta, agregação e movimentação de grandes volumes de dados de logs. |\n",
        "| Streaming        | Apache Kafka | Plataforma de streaming de eventos que permite publicar e consumir fluxos de dados em tempo real. |\n",
        "| Armazenamento    | Kudu         | Sistema de armazenamento distribuído otimizado para leituras e escritas rápidas, complementa HDFS para dados tabulares. |\n",
        "| Orquestração     | ZooKeeper    | Serviço centralizado para manutenção de informações de configuração, sincronização distribuída e grupos de nomeação. |\n",
        "| Consultas        | Apache Drill | Engine de consultas distribuídas, sem necessidade de esquemas, que suporta múltiplos tipos de dados (JSON, Parquet, etc.). |\n",
        "| Processamento    | Apache Pig   | Linguagem de alto nível para criar programas de análise de dados que são convertidos em MapReduce. |\n",
        "| Armazenamento    | Apache Solr  | Plataforma para busca e indexação de grandes volumes de dados, muitas vezes usada em conjunto com Hadoop. |\n"
      ],
      "metadata": {
        "id": "LA_5V6ZciJ1s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comercial Distributions\n",
        "\n",
        "* Cloudera (CDH - Cloudera Distribution for Hadoop) + Hortonworks (HDP - Hortonworks Data Platform)\n",
        "* MapR (agora HPE Ezmeral Data Fabric):\n",
        "* Amazon EMR (Elastic MapReduce)\n",
        "\n",
        "Mas **NÃO**,\n",
        "\n",
        "* ElasticSearch (emprega o Lucene como motor de indexação)\n",
        "* DataBricks (emprega o Spark, mas não o HDFS)\n",
        "* SnowFlake (trabalha sobre armazenamento em nuvem de diferentes provedores)"
      ],
      "metadata": {
        "id": "2AsCUGkdfryU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HDFS, Hadoop Distributed File System\n",
        "\n",
        "No passado recente, “Hadoop” era praticamente sinônimo de “big data”. O Hadoop\n",
        "Distributed File System é **baseado no Google File System (GFS)** e foi inicialmente projetado para processar dados com o modelo de programação MapReduce. O Hadoop é\n",
        "semelhante ao armazenamento de objetos, mas com uma diferença fundamental: **o Hadoop combina computação e\n",
        "armazenamento nos mesmos nós**, onde os armazenamentos de objetos normalmente têm suporte limitado para\n",
        "processamento interno.\n",
        "\n",
        "**O Hadoop divide arquivos grandes em blocos**, pedaços de dados com menos de algumas centenas\n",
        "megabytes de tamanho. O sistema de arquivos é gerenciado pelo NameNode, que mantém\n",
        "diretórios, metadados de arquivo e um catálogo detalhado descrevendo a localização dos blocos de arquivo\n",
        "no cluster. **Em uma configuração típica, cada bloco de dados é replicado para três\n",
        "nós. Isso aumenta a durabilidade e a disponibilidade dos dados.** Se um disco ou nó\n",
        "falha, o fator de replicação para alguns blocos de arquivo cairá abaixo de 3. O NameNode\n",
        "instruirá outros nós a replicar esses blocos de arquivo para que eles alcancem novamente o fator de\n",
        "replicação correto. Portanto, a probabilidade de perda de dados é muito baixa, exceto por uma\n",
        "falha correlacionada (por exemplo, um asteroide atingindo o data center).\n",
        "\n",
        "O Hadoop não é simplesmente um sistema de armazenamento. **O Hadoop combina recursos de computação** com\n",
        "nós de armazenamento para permitir o processamento de dados no local. Isso foi  alcançado **usando\n",
        "o modelo de programação MapReduce**.\n",
        "\n",
        "> **O Hadoop está morto?**\n",
        "\n",
        "> Isso é apenas parcialmente verdade. O Hadoop\n",
        "não é mais uma tecnologia quente e de ponta. Muitas ferramentas do ecossistema Hadoop, como o Apache Pig, agora estão em suporte de vida e são usadas principalmente para executar trabalhos legados. O modelo de programação MapReduce puro caiu no esquecimento. O HDFS continua amplamente\n",
        "usado em vários aplicativos e organizações.\n",
        "\n",
        "> O Hadoop **ainda aparece em muitas instalações legadas**. Além disso, o HDFS é um **ingrediente essencial de muitos mecanismos de big data atuais, como o\n",
        "Amazon EMR**. Na verdade, o **Apache Spark** ainda é comumente executado em clusters HDFS."
      ],
      "metadata": {
        "id": "nhKjXEvfjt4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/Rogerio-mack/data-engineering/blob/main/figs/HDFS.png?raw=true\" height=\"300\" align=\"left\"/></a>"
      ],
      "metadata": {
        "id": "uaG5tDRZx6cg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frameworks Computacionais\n",
        "\n",
        "Com os principais componentes do Hadoop, temos dados armazenados no HDFS e um meio de\n",
        "executar aplicativos distribuídos via YARN. Vamos ver\n",
        "analisar dois dos principais frameworks.\n",
        "\n"
      ],
      "metadata": {
        "id": "E2o6iJWmmgmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hadoop MapReduce\n",
        "MapReduce é o aplicativo original para o qual o Hadoop foi criado e **é uma implementação\n",
        "baseada em Java** do projeto estabelecido no artigo MapReduce do Google. Originalmente,\n",
        "era um framework autônomo em execução no cluster, mas foi posteriormente\n",
        "portado para o YARN conforme o projeto Hadoop evoluiu para oferecer suporte a mais aplicativos e casos de uso. Embora substituído por mecanismos mais novos, como Apache Spark e Apache\n",
        "Flink, ainda vale a pena entender, visto que **muitos frameworks de nível superior compilam\n",
        "suas entradas em trabalhos MapReduce para execução. Isso inclui o Hive, Pig e Sqoop**.\n",
        "\n",
        "Os termos map e reduce são emprestados da programação funcional,\n",
        "onde um map aplica uma função de transformação a cada elemento\n",
        "em uma coleção, e um reduce aplica uma função de agregação a\n",
        "cada elemento de uma lista, combinando-os em menos valores de resumo.\n",
        "\n",
        "Essencialmente, **o MapReduce divide uma computação em três estágios sequenciais: map,\n",
        "shuffle e reduce**. Na fase de map, os dados relevantes são lidos do HDFS e processados\n",
        "em paralelo por várias tarefas de map independentes. Essas tarefas devem ser executadas idealmente\n",
        "onde quer que os dados estejam localizados — geralmente buscamos uma tarefa de map por bloco HDFS. O\n",
        "usuário define uma função map() (em código) que processa cada registro no arquivo e produz\n",
        "saídas de valor-chave prontas para a próxima fase. Na fase de shuffle, as saídas do map\n",
        "são buscadas pelo MapReduce e enviadas pela rede para formar a entrada para as\n",
        "tarefas de reduce. Uma função reduce() definida pelo usuário recebe todos os valores para uma chave\n",
        "por sua vez e os agrega ou combina em menos valores que resumem as entradas.\n",
        "Os fundamentos do processo são mostrados na Figura 1-6. No exemplo, os arquivos são lidos\n",
        "do HDFS por mapeadores e embaralhados por chave de acordo com uma coluna de ID. Os redutores\n",
        "agregam as colunas restantes e gravam os resultados de volta no HDFS.\n",
        "\n",
        "**Sequências desses três estágios lineares simples podem ser compostas e combinadas em\n",
        "essencialmente qualquer computação de complexidade arbitrária; por exemplo, transformações avançadas,\n",
        "junções, agregações** e muito mais. Às vezes, para transformações simples que não\n",
        "requerem agregações, a fase de redução não é necessária. Normalmente, as saídas\n",
        "de um trabalho MapReduce são armazenadas de volta no HDFS, onde podem formar as entradas\n",
        "para outros trabalhos.\n",
        "\n",
        "Apesar de sua simplicidade, o MapReduce é incrivelmente poderoso e extremamente robusto e\n",
        "escalável. No entanto, ele tem algumas desvantagens. Primeiro, ele é bastante envolvido do\n",
        "ponto de vista de um usuário, que precisa codificar e compilar funções map() e reduce()\n",
        "em Java, o que é um nível muito alto para muitos analistas — compor pipelines de processamento\n",
        "complexos no MapReduce pode ser uma tarefa assustadora. Segundo, o MapReduce em si não é\n",
        "particularmente eficiente. Ele faz muita E/S baseada em disco, o que pode ser caro ao\n",
        "combinar estágios de processamento ou fazer operações iterativas. Pipelines multiestágio\n",
        "são compostos de trabalhos individuais do MapReduce com uma barreira de E/S HDFS\n",
        "entre eles, sem reconhecimento de otimizações potenciais em todo o gráfico de processamento\n",
        ".\n",
        "\n",
        "Devido suas desvantagens, vários sucessores do MapReduce foram desenvolvidos\n",
        "que visam simplificar o desenvolvimento e tornar os pipelines de processamento mais\n",
        "eficientes. Apesar disso, os fundamentos conceituais do MapReduce — que o processamento de dados\n",
        "deve ser dividido em várias tarefas independentes em execução em diferentes\n",
        "máquinas (mapas), cujos resultados são então embaralhados e agrupados e coletados\n",
        "juntos em outro conjunto de máquinas (redução) — são fundamentais para todos os mecanismos de processamento de dados\n",
        "distribuídos, incluindo estruturas baseadas em SQL. Apache Spark, Apache\n",
        "Flink e Apache Impala, embora todos bastante diferentes em suas especificidades, são essencialmente\n",
        "implementações diferentes desse conceito.\n",
        "\n"
      ],
      "metadata": {
        "id": "beiF6eqvm7AN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/Rogerio-mack/data-engineering/blob/main/figs/MapReduce.png?raw=true\" height=\"300\" align=\"left\"/></a>"
      ],
      "metadata": {
        "id": "CnozCUruyF13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apache Spark\n",
        "\n",
        "**Apache Spark é uma estrutura de computação distribuída, com ênfase em eficiência\n",
        "e usabilidade, que suporta computações em lote e streaming**. **Em vez\n",
        "de o usuário ter que expressar as manipulações de dados necessárias em termos de funções map()\n",
        "e reduce() puras como no MapReduce, o Spark expõe uma API rica de operações\n",
        "comuns, como filtragem, junção, agrupamento e agregações diretamente em Datasets,**\n",
        "que compreendem linhas todas aderindo a um tipo ou esquema específico. Além de usar métodos de API,\n",
        "os usuários podem enviar operações diretamente usando um **dialeto de estilo SQL** (daí o\n",
        "nome geral deste conjunto de recursos, Spark SQL), removendo grande parte da exigência\n",
        "de compor pipelines programaticamente.\n",
        "\n",
        "Com sua API, o Spark torna o trabalho de compor\n",
        "pipelines de processamento complexos muito mais tratável para o usuário. Como um simples\n",
        "exemplo, na Figura a seguir, três conjuntos de dados são lidos. Dois deles unidos e\n",
        "unidos com um terceiro conjunto de dados filtrado. O resultado é agrupado de acordo com uma coluna e\n",
        "agregado e gravado em uma saída.\n",
        "\n",
        "As fontes e coletores do conjunto de dados podem ser controlados por lote\n",
        "e usar HDFS ou Kudu, ou podem ser processados ​​em um fluxo de e para o Kafka.\n",
        "Um recurso essencial das operações em conjuntos de dados é que os gráficos de processamento são executados por meio de\n",
        "um otimizador de consulta padrão antes da execução, muito semelhante aos encontrados em bancos de dados relacionais\n",
        "ou em mecanismos de consulta de processamento massivamente paralelos. Este otimizador pode reorganizar,\n",
        "combinar e podar o **gráfico de processamento para obter o pipeline de execução\n",
        "mais eficiente**. Dessa forma, o mecanismo de execução pode operar em conjuntos de dados de uma forma muito\n",
        "mais eficiente, evitando grande parte da E/S intermediária da qual o MapReduce\n",
        "sofre.\n",
        "\n",
        "**Um dos principais objetivos de design do Spark era aproveitar ao máximo a memória\n",
        "em nós de trabalho, que está disponível em quantidades crescentes em servidores de commodities**.\n",
        "A capacidade de armazenar e recuperar dados da memória principal em velocidades que são ordens\n",
        "de magnitude mais rápidas do que as de discos giratórios torna certas cargas de trabalho radicalmente\n",
        "mais eficientes. Cargas de trabalho de aprendizado de máquina distribuídas em particular, que geralmente\n",
        "operam nos mesmos conjuntos de dados de forma iterativa, podem ver enormes benefícios em tempos de execução\n",
        "sobre a execução equivalente do MapReduce. O Spark permite que os conjuntos de dados sejam armazenados em cache na\n",
        "memória dos executores; se os dados não couberem inteiramente na memória, as partições\n",
        "que não podem ser armazenadas em cache são espalhadas no disco ou recomputadas de forma transparente no tempo de execução.\n",
        "\n",
        "**O Spark implementa o processamento de fluxo como uma série de microlotes periódicos de conjuntos de dados**.\n",
        "Essa abordagem requer apenas pequenas diferenças de código nas transformações e\n",
        "ações aplicadas aos dados — essencialmente, o mesmo código ou um código muito semelhante pode ser usado\n",
        "nos modos de lote e streaming. Uma desvantagem da abordagem de microlote é\n",
        "que ela leva pelo menos o intervalo entre os lotes para que um evento seja processado, portanto,\n",
        "não é adequada para casos de uso que exigem latências de milissegundos. No entanto, essa potencial\n",
        "fraqueza também é uma força porque o microlote permite uma taxa de transferência de dados muito maior\n",
        "do que pode ser alcançado ao processar eventos um por um. Em geral, há\n",
        "relativamente poucos casos de uso de streaming que realmente exigem tempos de resposta de subsegundos.\n",
        "No entanto, a funcionalidade de streaming estruturada do Spark promete trazer muitas das\n",
        "vantagens de um gráfico de computação em lote Spark otimizado para um contexto de streaming,\n",
        "bem como um modo de streaming contínuo de baixa latência.\n",
        "\n",
        "**O Spark envia uma série de bibliotecas e APIs integradas para aprendizado de máquina**. O Spark\n",
        "MLlib permite que o processo de criação de um modelo de aprendizado de máquina (preparação de dados,\n",
        "limpeza, extração de recursos e execução de algoritmo) seja composto em um pipeline distribuído. Nem todos os algoritmos de aprendizado de máquina podem ser executados automaticamente de\n",
        "maneira distribuída, então o Spark vem com algumas implementações de classes comuns de\n",
        "problemas, como clustering, classificação e regressão e filtragem colaborativa.\n",
        "\n",
        "Spark é uma estrutura extraordinariamente poderosa para processamento de dados e é frequentemente\n",
        "(corretamente) a escolha de fato ao criar novos casos de uso de processamento em lote, aprendizado de máquina\n",
        "e streaming. Mas não é o único jogo na cidade; arquitetos de aplicativos\n",
        "também devem considerar opções como **Apache Flink** para processamento em lote e streaming, e **Apache Impala** para SQL interativo."
      ],
      "metadata": {
        "id": "3N3aRQ6Qo94O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/Rogerio-mack/data-engineering/blob/main/figs/Spark.png?raw=true\" height=\"250\" align=\"left\"/></a>"
      ],
      "metadata": {
        "id": "gxSOswWryAwQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mecanismos analíticos de SQL\n",
        "\n",
        "Embora MapReduce e Spark sejam frameworks extremamente flexíveis e poderosos, para\n",
        "usá-los você precisa estar confortável programando em uma linguagem como Java, Scala,\n",
        "ou Python e deve estar feliz implantando e executando código da linha de comando.\n",
        "**A realidade é que, na maioria das empresas, SQL continua sendo a língua franca da análise,\n",
        "e a maior e mais acessível base de habilidades está lá.**  Às vezes, você precisa fazer as coisas\n",
        "sem a ladainha de codificar, compilar, implantar e executar um aplicativo completo.\n",
        "Além disso, um grande corpo de ferramentas de suporte à decisão e inteligência empresarial\n",
        "interagem com armazenamentos de dados exclusivamente por SQL. Por essas razões, muito tempo e\n",
        "esforço foram gastos desenvolvendo interfaces semelhantes a SQL para dados estruturados armazenados no\n",
        "Hadoop. Muitos deles usam MapReduce ou Spark como seu mecanismo de computação subjacente,\n",
        "mas alguns são mecanismos de computação por direito próprio. Cada mecanismo é\n",
        "focado em consultar dados que já existem no mecanismo de armazenamento ou em inserir novos\n",
        "dados em massa nesses mecanismos. Eles são projetados para análises em larga escala e não\n",
        "para processamento transacional em pequena escala. Vamos dar uma olhada nos principais participantes.\n",
        "\n",
        "### Apache Hive\n",
        "Apache Hive é a tecnologia original de data warehousing para Hadoop. Foi desenvolvido\n",
        "no Facebook e foi o primeiro a oferecer uma linguagem semelhante a SQL, chamada HiveQL, para\n",
        "permitir que analistas consultem dados estruturados armazenados no HDFS sem precisar primeiro compilar\n",
        "e implantar código. O Hive oferece suporte a conceitos comuns de consulta SQL, como junções de tabelas,\n",
        "uniões, subconsultas e visualizações. Em um alto nível, o Hive analisa uma consulta do usuário, otimiza-a\n",
        "e a compila em um ou mais cálculos em lote encadeados, que ele executa no\n",
        "cluster. Normalmente, esses cálculos são executados como trabalhos MapReduce, mas o Hive pode\n",
        "também usar Apache Tez e Spark como seu mecanismo de execução de apoio.  "
      ],
      "metadata": {
        "id": "lQgnnkyTtjP0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How-to\n",
        "\n",
        "* [How To HDFS Docker](https://colab.research.google.com/github/Rogerio-mack/data-engineering/blob/main/how_run_wordcount_example_on_docker_hadoop_not_Colab.ipynb)\n",
        "* [How To MapReduce HDFS Streaming Sum](https://colab.research.google.com/github/Rogerio-mack/data-engineering/blob/main/how_map_reducer_sum_hadoop_streaming_on_Colab.ipynb)\n",
        "> * [How To MapReduce HDFS Streaming ngrams](https://colab.research.google.com/github/Rogerio-mack/data-engineering/blob/main/how_map_reducer_ngrams_hadoop_streaming_on_Colab.ipynb)\n",
        "> * [How To MRjob](https://colab.research.google.com/github/Rogerio-mack/data-engineering/blob/main/how_map_reducer_sum_MRJob_on_Colab.ipynb)\n",
        "* [How To PySpark](https://colab.research.google.com/github/Rogerio-mack/data-engineering/blob/main/how_pyspark.ipynb)\n"
      ],
      "metadata": {
        "id": "-7Sspr6fF-Zc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercício (sala) - **Wikipedia Word Count**\n",
        "\n",
        "Em grupo de até 4 alunos. Empregue os exemplos de **PySpark** de word count para extrair os termos mais comumente utilizados em 4 temas (subjects) extraídos de páginas da wikipedia. Para obter os dados da Wikipedia, você pode utilizar [Get Wikipedia Content](https://colab.research.google.com/github/Rogerio-mack/data-engineering/blob/main/how_to_get_wikipediaContent_by_subject.ipynb). **Particione os dados pelos temas**. Faça a entrega do seu notebook, executável no Moodle.\n",
        "\n"
      ],
      "metadata": {
        "id": "_NJmpP0RL1kQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercício (home) - **How to Hadoop**\n",
        "\n",
        "Em grupo de até 4 alunos.\n",
        "\n",
        "1. Escolha uma das seguintes soluções do eco-sistema `Hadoop`:\n",
        "\n",
        "  1. Apache Spark Streaming\n",
        "  2. Apache Impala\n",
        "  2. Apache Pig\n",
        "  2. Apache Flume\n",
        "  2. Apache Solr\n",
        "\n",
        "2. Implemente, à exemplo das aulas, um **How to** para uma dessas tecnologias.\n",
        "\n",
        "3. Empregue em todos os casos dados do [GroupLens](https://grouplens.org/) ou, alternativamente dados texto extraídos da Wikipedia como no exercício anterior (particularmente útil para indexação do Solr). Para o problemas de Streaming, podem ser bases incrementadas do GroupLens ou streamings simuladas via Python.\n",
        "\n",
        "4. **How to READ** $\\times$ **How to Class**. Ao construir o seu How To fique atento a essa diferenças. Os empregados em sala são apresentados, já o do trabalho deve estar adequado à leitura! **Comentários, explicações, além da escolha dos casos de uso são essenciais e são uma parte importante da atividade.** Pense que seu tutorial poderia ser publicado.\n",
        "\n",
        "5. Sua implementação pode ser feita no ambiente Colab ou em um container, com instruções detalhadas para execução nos dois casos.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f-EcNU_mH4oA"
      }
    }
  ]
}